User 1: got into dsc?
User 2: ye me sarthak arijit
User 1: lol nice yo wish me at twelve
User 2: happy birthday dumbass  f ik u do this
User 1: so vasvi again from what i hear tellll have you taken ai
User 2: yep
User 1: who else
User 2: no idea
User 1: oye kaunsa theorbroma jaldi bata madarchod
User 2: o koi sa bhi jo mood kare
User 1: tere liye imaginary lauga
User 2: woh bhi chalega 
User 1: informed search is the last thing
User 2: my textbook doesn't have it's old lolz i had the first edition it's from third edition
User 1: apu
User 2: ye wat up
User 1: is naive bayes classifier linear or non linear and in it the number of training instances per class changes the output na what is this called? local something na like some technical word is it called class likelihood
User 2: log likelihood?
User 1: not used in bayes theorem right
User 2: bayes likelihood is different no prior, likelihood and estimator are the three things for bayes right
User 1: arre but in bayes there's a thing like right that if a particular class instances are more in the training set then that class is more likely to be predicted
User 2: that's because your prior is higher no
User 1: because the class likelihood is more? and this?
User 2: non linear
User 1: because it doesn't have any specific weighted sum of the inputs that's why right what elective are you selecting
User 2: cv
User 1: happy birthday apu
User 2: thanks bro 3
User 1: dude what model can i use where both input and output is text lstm or rnn only i'll have to us no
User 2: ya good choice
User 1: which gtx do you have
User 2: one thousand and sixty
User 1: and how many marks in pcap today
User 2: will get gud marks
User 1: seriously fuck do you have an internship?
User 2: ye i haz internship why u askin
User 1: where? cos i broke af nigga fucked for the vacation needed ideas to move forward or i'll end up wasting it.
User 2: rip worcester polytechnic institute
User 1: us meh hi rehja chutiya research i wanna start
User 2: :c
User 1: tell me the process tomorrow or whenever you free. and this i'm literally counting on you and seeing the start of the year, you please don't let my year go as shitty as the start love you thanks
User 2: wat what process
User 1: how to go about doing the research part. idk help
User 2: what'd you wanna know research isn't a stand alone field man the only way i encountered it was doing something to solve a specific problem and on the way i figured out a slightly different method that people hadn't tried yet or seemed like it wouldn't work took me 112 years to confirm it actually worked :
User 1: which was a tiny dataset, so your problem was how to get a larger training dataset and what things can be done? right?
User 2: not really no
User 1: did you publish it?
User 2: not yet, need more data moved my stack to tpus
User 1: nevermind then lol kiska bc
User 2: trying to speed up the process google free credits
User 1: you're going to continue with this at your internship? accha ha i recently used their gpus. too ez
User 2: i'll be using it in the internship will cite my own paper aw yis
User 1:  accha so over there you'll do something different?
User 2: a little different but overall very similar
User 1: ooo. so finally i'm tired of coursera. prolly not going to do more than a course or 2. already did ten so what should i do ? what i'm planning on for a couple of weeks is to perfect my older projects to tune them with whatever better i learned but i want do something worthwhile. especially that'll help me for my master's application any suggestions? you deleted this message boi. while selecting which model to pick in cv, should i select one with larger map or smaller val loss?
User 2: depends if evaluation is same for both cases, then this situation shouldn't arise it's not possible only if both were evaluated on the same thing if they're evaluated on different sets then check how many images it was actually evaluated on if there's slightly lower map for a significantly larger val set, choose that
User 1: it was for the same set. i didn't know this fact. basically trying to implement yolo.
User 2: i don't think it's possible smaller val loss should equal higher map unless there's a high class imbalance maybe nah actually even then it wouldn't be possible
User 1: oh i realized what i messed up. i mistook val loss for training loss nevermind map is calculated for only the training set right? or you can select?
User 2: map for val matters that's all you could calculate map for training but it has no significance really you could check if your model is underfit or overfit though high map for training, shit for eval means overfit af
User 1: accha cool cool thanks
User 2: bet exam kill me
User 1: killing you before that exam would have really saved you a lotta trouble tru
User 2: i was sick also should have ended me
User 1: why the fuck are you going home on the night of the photoshoot are you fucking retarded why you like this
User 2: gotta go home boi can't do anything
User 1: go on 7th??? anyway your going to reach on 7th
User 2: no man have a meeting on 7th morning that's why going on 6th
User 1: manas?
User 2: nah iisc
User 1: accha internship?
User 2: ye prof wanted to meet
User 1: akli baar skype ki id dedio
User 2: de chuka usko personal meeting chahiye
User 1: i hear that it's alright i'll miss you it's irritating how you're missing everything theek hai kar kar love you
User 2: happy birthday dickhead
User 1: thanks cunt when deciding the length of sentence to take for an input you have to truncate it right because all the sentence in the dataset are different size.
User 2: pad usually people will find max length of a sequence and pad everything else or you can pass nones too pad in batches but inter-batch can have variable lengths
User 1: how?
User 2: you'll have to write your own batching function for this group similar lengths together
User 1: this i understood but how will you input different length batches in the same model
User 2: tf.keras lets you you can pass none type for length in an lstm i think just cross-check once
User 1: oh oh okay i'll checkk 2019-11-03 23:26:35.772366: e tensorflowcoregrappleroptimizersmetaoptimizer.cc:533 modelpruner failed: invalid argument: mutablegraphview::mutablegraphview error: node 'losstimedistributed1lossmeansquarederrorweightedlossconcat' has self cycle fanin 'losstimedistributed1lossmeansquarederrorweightedlossconcat'. 2019-11-03 23:26:35.902219: e tensorflowcoregrappleroptimizersdependencyoptimizer.cc:697 iteration  0, topological sort failed with message: the graph couldn't be sorted in topological order. please to help thanks luv
User 2: aise kaise bolu boss this probably means you have a cycle in your graph
User 1: can i send you my model summary and code for archi it's pretty short and cute
User 2: do you have tf.nn activations anywhere
User 1: i'm using keras and in that i'm giving activations
User 2: wb concat are you using it anywhere
User 1: repeatvector i'm using
User 2: which tf version are you using could try rolling back the version
User 1: i'm running it on collab anyway which version to use
User 2: any previous version
User 1: cool i'll check something thanks
User 2: 1.16 or sumthn actually no try downgrading all the way to one error might be because of an activation preceded by a concat tho apparently
User 1: mind explaining
User 2: idk man i googled they said iz vug bug who knos
User 1: loll even i didn't get it
User 2: why'd you call
User 1: i was harshas place only. thought i'd swing by and say hi and offer you some pizza
User 2: oh rip i was out cold
User 1: it wasn't even ten
User 2: hadn't slept had end sem lab so instead of studying was watching videos all night
User 1: how did your exam go bro i got 1030 in execution
User 2: wtf xd how do you know you got 1030 mine went well i think, got the output
User 1: we had marks written on our paperm so she ticked next to ten marks lel full no?
User 2: mayb
User 1: toh full only youre getting chut
User 2: write galat hai xd write up
User 1: madarchod you never told me you're dating vasvi! sorry for being so mean
User 2: relx man
User 1: bolhi deta pehle you in bangalore only no?
User 2: ye visa hasn't come yet
User 1: werent you doing iisc only?
User 2: nono iisc till my visa came
User 1: ohh otherwise where you going
User 2: i'm going to ntu in feb
User 1: ooooo okay then we drink this weekend then.
User 2: bastard  how many times have you already gotten drunk in bangalore
User 1: no i'm flying on saturday i've never been to blore  new decade new me i've changed 
User 2: fuck off  how many times have you gotten drunk already regardless of where you were
User 1: zero well since my mom caught my pot stash at home  yeah man manipal drama is following up like always 
